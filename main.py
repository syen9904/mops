import httpx
from bs4 import BeautifulSoup
import json
import asyncio
import re
from datetime import datetime, timezone, timedelta
from pathlib import Path

COMPANIES_FILE = "companies.txt"
STATE_FILE = "state.json"
README_FILE = "README.md"


def load_companies() -> list[str]:
    """è®€å– companies.txtï¼Œå›žå‚³ [ä»£è™Ÿ, ...]"""
    companies = []
    for line in Path(COMPANIES_FILE).read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if line and not line.startswith("#"):
            companies.append(line)
    return companies


def load_state() -> dict:
    """è®€å–ä¸Šæ¬¡çš„æ—¥æœŸè¨˜éŒ„"""
    if Path(STATE_FILE).exists():
        return json.loads(Path(STATE_FILE).read_text(encoding="utf-8"))
    return {}


def save_state(state: dict):
    Path(STATE_FILE).write_text(
        json.dumps(state, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


def generate_readme(companies: list[str], state: dict, updates: list[str], names: dict):
    """ç”Ÿæˆæ˜“è®€çš„ README.md"""
    taipei_tz = timezone(timedelta(hours=8))
    now = datetime.now(taipei_tz).strftime("%Y/%m/%d %H:%M")

    # å…ˆæŒ‰æ›´æ–°æ—¥æœŸï¼ˆæ–°åˆ°èˆŠï¼‰ï¼Œå†æŒ‰æ³•èªªæœƒæ—¥æœŸï¼ˆèˆŠåˆ°æ–°ï¼‰
    def sort_key(co_id):
        data = state.get(co_id, {})
        if isinstance(data, dict):
            updated = data.get("updated", "")
            content = data.get("content", "")
        else:
            updated = ""
            content = ""
        # æ›´æ–°æ—¥æœŸåè½‰ï¼ˆæ–°çš„åœ¨å‰ï¼‰ï¼Œæ³•èªªæœƒæ—¥æœŸæ­£å¸¸ï¼ˆèˆŠçš„åœ¨å‰ï¼‰
        return (-int(updated.replace("/", "")) if updated else 0, content)

    sorted_companies = sorted(companies, key=sort_key)

    lines = [
        "# æ³•èªªæœƒè¿½è¹¤",
        "",
        "built by [å°åš´](https://linkedin.com/in/syen9904)",
        "",
        f"æœ€å¾ŒåŸ·è¡Œï¼š{now} (UTC+8)",
        "",
    ]

    if updates:
        lines.append("## ðŸ”” æœ¬æ¬¡æ›´æ–°")
        lines.append("")
        for u in updates:
            lines.append(f"- {u}")
        lines.append("")

    lines.append("## è¿½è¹¤æ¸…å–®")
    lines.append("")
    lines.append("- æ¯æ—¥ç›¤å‰ç›¤å¾Œè‡ªå‹•æ›´æ–°ï¼ˆç¢ºåˆ‡æ™‚é–“ä»¥ä¼ºæœå™¨æŽ’ç¨‹ç‚ºä¸»ï¼‰")
    lines.append("- æŽ’åºï¼šæ›´æ–°æ—¥æœŸï¼ˆæ–°â†’èˆŠï¼‰ï¼Œå†ä¾æ³•èªªæœƒé–‹å§‹æ—¥æœŸï¼ˆèˆŠâ†’æ–°ï¼‰")
    lines.append("")
    lines.append("| ä»£è™Ÿ | å…¬å¸ | æ³•èªªæœƒæ—¥æœŸ | æ›´æ–°æ—¥æœŸ |")
    lines.append("|------|------|-----------|---------|")

    for co_id in sorted_companies:
        name = names.get(co_id, co_id)
        data = state.get(co_id, {})
        if isinstance(data, dict):
            content = data.get("content", "-")
            updated = data.get("updated", "-")
        else:
            content = data if data else "-"
            updated = "-"
        lines.append(f"| {co_id} | {name} | {content} | {updated} |")

    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("è¦æ–°å¢žå…¬å¸ï¼Ÿç·¨è¼¯ `companies.txt`ï¼Œä¸€è¡Œä¸€å€‹ä»£è™Ÿ")

    Path(README_FILE).write_text("\n".join(lines), encoding="utf-8")


async def fetch_content(client: httpx.AsyncClient, co_id: str) -> tuple[str | None, str | None]:
    """å¾ž HTML æŠ“å…¬å¸åç¨±å’Œæ³•èªªæœƒæ—¥æœŸï¼Œå›žå‚³ (name, date)"""
    try:
        res = await client.post(
            "https://mopsov.twse.com.tw/mops/web/ajax_t100sb07_1",
            data={
                "encodeURIComponent": "1",
                "step": "1",
                "firstin": "true",
                "off": "1",
                "TYPEK": "all",
                "co_id": co_id,
            },
        )
        # è¢« rate limit å°±ç­‰ 30 ç§’å†è©¦
        if "Overrun" in res.text:
            print(f"  {co_id}: RATE LIMITED, sleeping 30s...")
            await asyncio.sleep(30)
            return await fetch_content(client, co_id)

        soup = BeautifulSoup(res.text, "html.parser")

        # æŠ“å…¬å¸åç¨±ï¼ˆåœ¨ "å…¬å¸åç¨±ï¼š" å¾Œé¢ï¼‰
        name = None
        text = soup.get_text()
        if "å…¬å¸åç¨±ï¼š" in text:
            name = text.split("å…¬å¸åç¨±ï¼š")[1].split()[0].strip()

        # æŠ“æ³•èªªæœƒæ—¥æœŸ
        date = None
        for tr in soup.select("tr"):
            if "å¬é–‹æ³•äººèªªæ˜Žæœƒæ—¥æœŸ" in tr.get_text():
                td = tr.select("td")[-1] if tr.select("td") else None
                if td:
                    blues = [f.get_text().strip() for f in td.select("font[color=blue]")]
                    dates = [b for b in blues if "/" in b and len(b) >= 8]
                    if len(dates) >= 2:
                        start, end = dates[0], dates[1]
                        date = start if start == end else f"{start} ~ {end[4:]}"
                    elif dates:
                        date = dates[0]
                break

        return name, date
    except Exception as e:
        print(f"Error fetching {co_id}: {e}")
        return None, None


async def main():
    companies = load_companies()
    state = load_state()
    print(f"è¿½è¹¤ {len(companies)} é–“å…¬å¸")

    updates = []
    names = {}  # co_id -> name (stateless)

    today = datetime.now().strftime("%Y/%m/%d")

    async with httpx.AsyncClient(timeout=30, verify=False) as client:
        for co_id in companies:
            old = state.get(co_id, {})
            old_content = old.get("content", "") if isinstance(old, dict) else old

            name, new_content = await fetch_content(client, co_id)
            names[co_id] = name or co_id

            if new_content and new_content != old_content:
                updates.append(f"**{co_id} {name or ''}**")
                state[co_id] = {"content": new_content, "updated": today}
                print(f"  {co_id} {name}: UPDATED")
            else:
                print(f"  {co_id} {name}: no change")

            await asyncio.sleep(0.1)

    save_state(state)
    generate_readme(companies, state, updates, names)

    if updates:
        print(f"\næ›´æ–° {len(updates)} ç­†")
    else:
        print("\nç„¡æ›´æ–°")


if __name__ == "__main__":
    asyncio.run(main())
